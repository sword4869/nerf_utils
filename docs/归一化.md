- [1. 向量的范数](#1-向量的范数)
- [2. 向量的长度](#2-向量的长度)
- [3. 向量单位化](#3-向量单位化)
- [4. 归一化](#4-归一化)
  - [4.1. 最大最小归一化 Min-Max Normalization](#41-最大最小归一化-min-max-normalization)
  - [4.2. z-score 标准化](#42-z-score-标准化)


## 1. 向量的范数

- 0范数：向量中的最大值。
- p范数：$\sqrt[p]{\sum (|v_i|^p) }$
  - 1范数：
    向量的绝对值之和。$\sum |v_i|$
  - 2范数：
    向量的平方和的平方根。$\sqrt{\sum (|v_i|^2) }$
    别称，Euclidean norm, 向量的长度
  - ...

## 2. 向量的长度

向量的长度，叫做的向量的模 $|u|$。向量的模，即是算向量的二范数$\|u\|_2$。

$|u|=\|u\|=\|u\|_2$，都是同一种表示。


## 3. 向量单位化 

A vector that has a magnitude of 1 is a **unit vector**.  It is also known as **direction vector**.

向量除以向量的模，得到和这个向量方向相同的单位向量。

$\hat{u} = \dfrac{u}{\|u\|}$

```python
def normalize_vec(x:np.ndarry):
    return x / np.linalg.norm(x)
```
```python
# torch.nn.functional.normalize(input, p=2.0, dim=1, eps=1e-12, out=None)
# 需要输入是 float
x = torch.tensor([2, 2, 2, 2], dtype=torch.float32)
torch.nn.functional.normalize(x, dim=0)
```

## 4. 归一化

- 归一化在  [0.0, 1.0] 之间是统计的概率分布， 归一化在 [-1.0, 1.0] 之间是统计的坐标分布


- 输入数据的量纲不同时，消除不同的量纲单位的影响。因为入到神经网络中可不带单位，神经网络可认为大家单位都一样。

  ```
  1000(nm)，1(km)
  2000(nm)，1.1(km)
  ```
- 消除各维度的范围影响，加快了梯度下降求最优解的速度。

  ![椭圆与圆](https://img-blog.csdnimg.cn/2019072716003845.png)

### 4.1. 最大最小归一化 Min-Max Normalization

是对原始数据的线性变换
```python
def normalize(x: np.ndarry):
    min = arr.min()
    max = arr.max()

    # [0.0, 1.0]
    x = (x - min) / (max - min)

    # [-1.0, 1.0]
    x = (x - min) / (max - min) * 2 - 1.0
    return x
```



### 4.2. z-score 标准化

$z=\dfrac{x-\mu}{\sigma}$
z 为经过转换后的 z-score，μ 为总体样本空间的分值均值，σ 则为总体样本空间的标准差。
```python
def normalize(x: np.ndarry):
    x = (x - x.mean) / x.std()
    return x
```
```python
transforms.Normalize(mean=mean, std=std)
```